{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\"We run a Q&A forum with 27.007 members and would like to keep track of our members’ activity \n",
    "(i.e. how many posts/comments do they submit to our forum). We would like to send out a mail to \n",
    "those who are becoming less active on our platform, in the hope they start using it more.\"\n",
    "\n",
    "Ok, So the goal is to improve customer activity. Normally we only needed to extract some data from the csv-files and store them in a database and that\n",
    "was the whole assigment. But I've done that in an afternoon so that shit is done already. Let's do\n",
    "some showoff expansions just omda het kan. \n",
    "\n",
    "Showoff idea's\n",
    "- Run that shit on an AWS cluster\n",
    "- Do some crazy machine learning shit\n",
    "- Dive into the data a bit further to do some analysis \n",
    "    - I saw that the person_0 file has personal identifiable data. Might be interesting to \n",
    "    predict that for example mainly users of Firefox are inactive users.\n",
    "    - Hypotheses we could test: \n",
    "        - Browser, male/female, location (and show off our knowledge of ip- addresses, we might even go completely crazy and use \n",
    "        a public API for this), age, creationDate\n",
    "        - Mail-provider (and show off our knowledge of regexes\n",
    "        - Time of day a comment is posted\n",
    "    - Go completely nuts and use NLP on the content (maybe a bit overkill...)\n",
    "- Instead of using a local database using one via AWS, also more realistic in an actual production environment.\n",
    "       \n",
    " TODO: transfer to EMR want mijn computertje ontploft bijna (refactor to a .py file so it can be run on EMR)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "import mysql.connector \n",
    "\n",
    "\n",
    "database_password = input(\"Your database password please my good sir\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "data": {
      "text/plain": "b'java version \"1.8.0_241\"\\nJava(TM) SE Runtime Environment (build 1.8.0_241-b07)\\nJava HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode)\\n'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 146
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "Yeah, don't mind this. It is just for myself to check if I have the correct java version activated.\n",
    "\n",
    "source ~/.bashrc\n",
    "source ~/.bash_profile\n",
    "\n",
    "uncomment the following line in /.bash_profile\n",
    " - export JAVA_HOME=$(/usr/libexec/java_home -v 1.8) pyspark\n",
    "'''\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = '/Library/Java/JavaVirtualMachines/jdk1.8.0_241.jdk/Contents/Home'\n",
    "\n",
    "import subprocess\n",
    "subprocess.check_output(['java', '-version'], stderr=subprocess.STDOUT)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Basics\").getOrCreate()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Join the tables in the file comment_hasCreator_person_0_0 with the data in the file comment_0_0 on the comment's id. This way you have per person (i.e. his/her id) the set of comments/posts that he/she has made and the corresponding date of that post.\n",
    "\n",
    "    Use Spark's join functionality for this.\n",
    "\n",
    "   "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "#Reading in data\n",
    "comment_ids = spark.read.csv('../forum/comment_hasCreator_person_0_0.csv', sep='|', header=True,inferSchema=True)\n",
    "comment_list = spark.read.csv('../forum/comment_0_0.csv', sep='|', header=True, inferSchema=True)\n",
    "mail = spark.read.csv('../forum/person_email_emailaddress_0_0.csv', sep='|', header=True,inferSchema=True)\n",
    "person = spark.read.csv('../forum/person_0_0.csv', sep='|', header=True,inferSchema=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "comment_ids.printSchema()\n",
    "comment_list.printSchema()\n",
    "mail.printSchema()\n",
    "person.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   },
   "execution_count": 149,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "root\n",
      " |-- Comment.id: long (nullable = true)\n",
      " |-- Person.id: long (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- creationDate: string (nullable = true)\n",
      " |-- locationIP: string (nullable = true)\n",
      " |-- browserUsed: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- length: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Person.id: long (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birthday: timestamp (nullable = true)\n",
      " |-- creationDate: string (nullable = true)\n",
      " |-- locationIP: string (nullable = true)\n",
      " |-- browserUsed: string (nullable = true)\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "# print(\"Length of comment: \", comment_ids.count())\n",
    "# print(\"Length of comment_list: \", comment_list.count())\n",
    "# print(\"Length of mail: \", mail.count())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\n\\nLets look what's in this data. Uncomment all show() thingies when you want to\\nsee what is in each Dataframe. For now I have commented out all these methods \\nbecause they make our program run quite slowly and printing all these things \\nout also makes our output very unclear.\\n\\n\""
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 151
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Lets look what's in this data. Uncomment all show() thingies when you want to\n",
    "see what is in each Dataframe. For now I have commented out all these methods \n",
    "because they make our program run quite slowly and printing all these things \n",
    "out also makes our output very unclear.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# comment_ids.show()\n",
    "# comment_list.show()\n",
    "# mail.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "'''\n",
    "Renaming column name where we are going to join on because spark does not \n",
    "like dots and capital letters that much\n",
    "'''\n",
    "comment_ids = comment_ids.withColumnRenamed('Comment.id','comment_id')\n",
    "comment_ids = comment_ids.withColumnRenamed('Person.id','person_id')\n",
    "\n",
    "mail = mail.withColumnRenamed('Person.id','person_id')\n",
    "\n",
    "comment_list = comment_list.withColumnRenamed('id','comment_id')\n",
    "comment_list = comment_list.withColumnRenamed('creationDate','creationDate_comment')\n",
    "\n",
    "\n",
    "person = person.withColumnRenamed('id','person_id')\n",
    "person = person.withColumnRenamed('creationDate','creationDate_person')\n",
    "\n",
    "# comment_ids.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [],
   "source": [
    "#Converting to appropriate datatypes\n",
    "\n",
    "comment_ids = comment_ids.withColumn('comment_id', comment_ids['comment_id'].cast('int'))\n",
    "comment_ids = comment_ids.withColumn('person_id', comment_ids['person_id'].cast('int'))\n",
    "\n",
    "comment_list = comment_list.withColumn('comment_id', comment_list['comment_id'].cast('int'))\n",
    "comment_list = comment_list.withColumn('creationDate_comment', comment_list['creationDate_comment'].cast('timestamp'))\n",
    "\n",
    "mail = mail.withColumn('person_id', mail['person_id'].cast('int'))\n",
    "\n",
    "person = person.withColumn('person_id', person['person_id'].cast('int'))\n",
    "person = person.withColumn('creationDate_person', person['creationDate_person'].cast('timestamp'))\n",
    "person = person.withColumn('birthday', person['birthday'].cast('date'))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+----------+---------+\n",
      "|comment_id|person_id|\n",
      "+----------+---------+\n",
      "|         1|    20605|\n",
      "|         2|    11700|\n",
      "|        87|     9357|\n",
      "|        88|     3329|\n",
      "|        89|     9357|\n",
      "|        90|    14959|\n",
      "|        91|    14959|\n",
      "|        92|     9363|\n",
      "|        93|    25890|\n",
      "|        94|     1448|\n",
      "|        95|     9357|\n",
      "|        96|     9363|\n",
      "|        97|     7997|\n",
      "|        98|     7989|\n",
      "|        99|    27589|\n",
      "|       100|    29082|\n",
      "|       101|     7997|\n",
      "|       102|     7997|\n",
      "|       103|    17781|\n",
      "|       104|    17732|\n",
      "+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+--------------------+---------------+-----------------+--------------------+------+\n",
      "|comment_id|creationDate_comment|     locationIP|      browserUsed|             content|length|\n",
      "+----------+--------------------+---------------+-----------------+--------------------+------+\n",
      "|         1|2019-08-12 07:20:...|    24.246.24.0|          Firefox|                 yes|     3|\n",
      "|         2|2019-08-12 04:04:...|   196.1.96.246|           Chrome|              thanks|     6|\n",
      "|        87|2019-11-09 20:46:...| 170.210.104.44|          Firefox|                 LOL|     3|\n",
      "|        88|2019-11-10 05:55:...|112.137.174.133|           Chrome|               I see|     5|\n",
      "|        89|2019-11-09 18:32:...| 170.210.104.44|          Firefox|                fine|     4|\n",
      "|        90|2019-11-09 19:21:...|   201.240.7.55|           Chrome|               right|     5|\n",
      "|        91|2019-11-09 20:21:...|   201.240.7.55|           Chrome|About J. R. R. To...|    88|\n",
      "|        92|2019-11-09 21:37:...| 31.149.166.181|          Firefox|                good|     4|\n",
      "|        93|2019-11-10 02:41:...|     36.0.7.237|Internet Explorer|                  no|     2|\n",
      "|        94|2019-11-10 15:15:...|118.107.125.235|           Chrome|About J. R. R. To...|    87|\n",
      "|        95|2019-11-10 16:17:...| 170.210.104.44|          Firefox|                cool|     4|\n",
      "|        96|2019-11-10 17:53:...| 31.149.166.181|          Firefox|About Kelly Clark...|   104|\n",
      "|        97|2019-11-11 01:19:...|  49.246.190.67|           Chrome|                good|     4|\n",
      "|        98|2019-11-09 22:48:...|  200.60.47.249|          Firefox|                 thx|     3|\n",
      "|        99|2019-11-09 18:20:...|  14.192.60.180|           Chrome|About J. R. R. To...|    81|\n",
      "|       100|2019-11-10 15:25:...|     1.1.23.149|           Safari|                good|     4|\n",
      "|       101|2019-11-09 19:32:...|  49.246.190.67|           Chrome|                 duh|     3|\n",
      "|       102|2019-11-10 15:19:...|  49.246.190.67|           Chrome|About J. R. R. To...|    80|\n",
      "|       103|2019-11-09 21:31:...|103.246.104.175|           Chrome|                fine|     4|\n",
      "|       104|2019-11-10 03:18:...|    190.6.191.8|          Firefox|About Edward I of...|    89|\n",
      "+----------+--------------------+---------------+-----------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+--------------------+\n",
      "|person_id|               email|\n",
      "+---------+--------------------+\n",
      "|      461|    Eli461@gmail.com|\n",
      "|      461|    Eli461@yours.com|\n",
      "|      933|Mahinda933@boarde...|\n",
      "|      933|Mahinda933@hotmai...|\n",
      "|      933|Mahinda933@yahoo.com|\n",
      "|      933| Mahinda933@zoho.com|\n",
      "|     1063|Gustavo1063@gmail...|\n",
      "|     1063| Gustavo1063@gmx.com|\n",
      "|     1129|Carmen1129@gmail.com|\n",
      "|     1129|Carmen1129@yahoo.com|\n",
      "|     1132|    A.1132@gmail.com|\n",
      "|     1132|      A.1132@gmx.com|\n",
      "|     3861|     Ida3861@gmx.com|\n",
      "|     3861| Ida3861@hotmail.com|\n",
      "|     3861|   Ida3861@yahoo.com|\n",
      "|     4194|  Ho.Chi4194@gmx.com|\n",
      "|     4891| Elmar4891@gmail.com|\n",
      "|     4891|   Elmar4891@gmx.com|\n",
      "|     4891|Elmar4891@hotmail...|\n",
      "|     5315|Alfred5315@hotmai...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "comment_ids.show()\n",
    "comment_list.show()\n",
    "mail.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "#We only need the 'id' and 'creationDate' columns so we'll drop the rest\n",
    "\n",
    "comment_list = comment_list.select('comment_id','creationDate_comment')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "#Joining the tables together\n",
    "\n",
    "first_join = comment_ids.join(comment_list, on=['comment_id'], how= 'inner')\n",
    "# first_join.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "join the previous result with the data in the file person_email_emailaddress_0_0 on the person's id.\n",
    "\n",
    "As the file with emails is quite small, you could do the join by making a dictionary that maps each person id to one of its email addresses and broadcast that dictionary to each mapper and do the join yourself. Or you can again make use of Spark's join functionality.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "second_join = first_join.join(mail, on=[\"person_id\"], how='inner')\n",
    "# second_join.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "# I might have to change person and second_join here again\n",
    "\n",
    "final_join = person.join(second_join, on=[\"person_id\"], how='inner')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "data": {
      "text/plain": "\"\\nComment these lines if you don't want to get Rick rolled and learn the danger of running \\ncode without understanding what it does or reading the documentation.\\n\""
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 139
    }
   ],
   "source": [
    "'''\n",
    "Comment these lines if you don't want to get Rick rolled and learn the danger of running \n",
    "code without understanding what it does or reading the documentation.\n",
    "'''\n",
    "\n",
    "# import webbrowser\n",
    "# webbrowser.open('https://www.youtube.com/watch?v=dQw4w9WgXcQ') "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Group this set by the person's email and month (in which the comment was created) and count the number of comments.\n",
    "So per person you should now have a record in the form of Person.email|Month|Number of comments\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [],
   "source": [
    "# Lets first look what's exactly in this 'creationDate' column\n",
    "\n",
    "# final_join.select('creationDate').show()\n",
    "# final_join.select('creationDate').head(5)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "'''\n",
    "Extracting the month from the 'creationDate_comment' column. \n",
    "Please note that these are in numerical format (E.g. 1 = \"January\")\n",
    "'''\n",
    "\n",
    "from pyspark.sql.functions import month\n",
    "final_join = final_join.withColumn(\"creationDate_comment_month\", month(final_join['creationDate_comment']))\n",
    "# final_join= final_join.withColumn('creationDate_DT',functions.to_date(final_join.creationDate))\n",
    "# final_join.printSchema()\n",
    "# final_join.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [],
   "source": [
    "feature_group = ['email','creationDate_comment_month']\n",
    "final_join_short = final_join.groupBy(feature_group).count().withColumnRenamed('count', 'Number_of_comments_that_month')\n",
    "final_join = final_join.join(final_join_short, feature_group)\\\n",
    "    .orderBy([\"email\", 'creationDate_comment_month']).dropDuplicates()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [],
   "source": [
    "# final_join.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "'''\n",
    "Ok, this is going to be hard but it will be worth it, pinky promise!\n",
    "\n",
    "- Dowload the jbdc connector here (choose 'Platform independent'): https://dev.mysql.com/downloads/connector/j/\n",
    "- Unpack the zip file\n",
    "\n",
    "- sudo mv ~/Downloads/mysql-connector-java-8.0.19 /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyspark/jars\n",
    "(change according to your own locations of course)\n",
    "\n",
    "if this is too hard: use the code from, much longer, solution called \"old database\" \n",
    "but be warned: collect() might not work if you use it on your complete dataset. \n",
    "'''\n",
    "\n",
    "#This might take a few minutes to run... What we are doing now is pushing the whole dataset into our database. \n",
    "\n",
    "mode = 'overwrite'\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": database_password,\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "final_join_short.write.jdbc(url='jdbc:mysql://localhost/forum?serverTimezone=UTC', table=\"mail\", mode=mode, properties=properties)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "execution_count": 144,
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-4202695faee6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m }\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mfinal_join_short\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'jdbc:mysql://localhost/forum?serverTimezone=UTC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mail\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m             \u001b[0mjprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1041.jdbc.\n: java.sql.SQLException: Access denied for user 'root'@'localhost' (using password: NO)\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\n\tat com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:836)\n\tat com.mysql.cj.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:456)\n\tat com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:246)\n\tat com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:197)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverWrapper.connect(DriverWrapper.scala:45)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:63)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:54)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:515)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ],
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1041.jdbc.\n: java.sql.SQLException: Access denied for user 'root'@'localhost' (using password: NO)\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)\n\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:97)\n\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\n\tat com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:836)\n\tat com.mysql.cj.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:456)\n\tat com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:246)\n\tat com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:197)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverWrapper.connect(DriverWrapper.scala:45)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:63)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:54)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:48)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:515)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# final_join.count()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sampling Dataframe for data Analysis\n",
    "sample = final_join.sample(0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sample.count()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample.write.save(\"data/final_join.parquet\", mode='overwrite')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# checking if person_id can have multiple entries, which it can\n",
    "sample.createOrReplaceTempView(\"table1\")\n",
    "spark.sql(\"select * from table1 where person_id = 15870\").show()\n",
    "\n",
    "'''\n",
    "So this is stupid: our assignment specifies that we need to\n",
    "\"group this set by the person’s email and month\", but we now\n",
    "also know that a person can have multiple emailaddresses, \n",
    "which can screw up our interpretation. It would be better\n",
    "to group on person_id\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's it, We'll do our Data Analysis in another file.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-66ecb5d2",
   "language": "python",
   "display_name": "PyCharm (Python)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}